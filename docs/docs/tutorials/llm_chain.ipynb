{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wojiushigexiaobai/Myblog/blob/main/docs/docs/tutorials/llm_chain.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "raw",
      "id": "63ee3f93",
      "metadata": {
        "id": "63ee3f93"
      },
      "source": [
        "---\n",
        "sidebar_position: 0\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9316da0d",
      "metadata": {
        "id": "9316da0d"
      },
      "source": [
        "# Build a simple LLM application with chat models and prompt templates\n",
        "# 使用聊天模型和提示模板构建一个简单的LLM应用程序\n",
        "在本快速入门中，我们将向您展示如何使用 LangChain 构建一个简单的LLM应用程序。此应用程序会将文本从英语翻译成另一种语言。这是一个相对简单的LLM应用程序 - 它只是一个LLM调用加上一些提示。尽管如此，这是开始使用 LangChain 的好方法 - 只需一些提示和调用LLM就可以构建很多功能！\n",
        "\n",
        "阅读本教程后，您将对以下内容有一个大致的了解：\n",
        "\n",
        "- 使用 [语言模型](/docs/concepts/chat_models)\n",
        "\n",
        "- 使用 [提示模板](/docs/concepts/prompt_templates)\n",
        "\n",
        "- 使用[LangSmith]调试和跟踪应用程序 (https://docs.smith.langchain.com/)\n",
        "\n",
        "让我们开始吧！\n",
        "\n",
        "## 设置\n",
        "\n",
        "### Jupyter Notebook\n",
        "\n",
        "This and other tutorials are perhaps most conveniently run in a [Jupyter notebooks](https://jupyter.org/). Going through guides in an interactive environment is a great way to better understand them. See [here](https://jupyter.org/install) for instructions on how to install.\n",
        "\n",
        "本教程和其他教程可能在 Jupyter 笔记本中运行最方便。在交互式环境中浏览指南是更好地了解它们的好方法。有关如何安装的说明，请参阅此处。\n",
        "\n",
        "### Installation 安装\n",
        "\n",
        "要安装 LangChain，请运行：\n",
        "\n",
        "import Tabs from '@theme/Tabs';\n",
        "import TabItem from '@theme/TabItem';\n",
        "import CodeBlock from \"@theme/CodeBlock\";\n",
        "\n",
        "<Tabs>\n",
        "  <TabItem value=\"pip\" label=\"Pip\" default>\n",
        "    <CodeBlock language=\"bash\">pip install langchain</CodeBlock>\n",
        "  </TabItem>\n",
        "  <TabItem value=\"conda\" label=\"Conda\">\n",
        "    <CodeBlock language=\"bash\">conda install langchain -c conda-forge</CodeBlock>\n",
        "  </TabItem>\n",
        "</Tabs>\n",
        "\n",
        "\n",
        "\n",
        "有关更多详细信息，请参阅我们的[安装指南](/docs/how_to/installation).\n",
        "\n",
        "### LangSmith\n",
        "\n",
        "您使用 LangChain 构建的许多应用程序将包含多个步骤和多次调用LLM。随着这些应用程序变得越来越复杂，能够检查您的链条或代理内部到底发生了什么变得至关重要。最好的方法是使用[LangSmith](https://smith.langchain.com).\n",
        "\n",
        "在上面的链接中注册后，请确保设置环境变量以开始记录跟踪：\n",
        "\n",
        "```shell\n",
        "export LANGSMITH_TRACING=\"true\"\n",
        "export LANGSMITH_API_KEY=\"...\"\n",
        "```\n",
        "\n",
        "或者，如果在笔记本中，您可以通过以下方式设置它们：\n",
        "\n",
        "```python\n",
        "import getpass\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U langchain langchain-openai"
      ],
      "metadata": {
        "id": "Whqt_vea0RyT"
      },
      "id": "Whqt_vea0RyT",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "os.environ[\"LANGSMITH_TRACING\"] = \"true\"\n",
        "# os.environ[\"LANGSMITH_API_KEY\"] = getpass.getpass()\n",
        "os.environ[\"LANGSMITH_API_KEY\"] = userdata.get('LANGSMITH_API_KEY')"
      ],
      "metadata": {
        "id": "DpcLFiwl0j5J"
      },
      "id": "DpcLFiwl0j5J",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "LANGSMITH_TRACING=true\n",
        "LANGSMITH_ENDPOINT=\"https://api.smith.langchain.com\"\n",
        "LANGSMITH_API_KEY=\"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\"\n",
        "LANGSMITH_PROJECT=\"pr-terrible-halibut-84\"\n",
        "OPENAI_API_KEY=\"<your-openai-api-key>\""
      ],
      "metadata": {
        "id": "wk_7_QO30WTD"
      },
      "id": "wk_7_QO30WTD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "e5558ca9",
      "metadata": {
        "id": "e5558ca9"
      },
      "source": [
        "## Using Language Models 使用语言模型\n",
        "\n",
        "首先，让我们学习如何单独使用语言模型。LangChain 支持许多不同的语言模型，您可以互换使用。有关开始使用特定模型的详细信息，请参阅[支持的集成](/docs/integrations/chat/).\n",
        "\n",
        "import ChatModelTabs from \"@theme/ChatModelTabs\";\n",
        "\n",
        "<ChatModelTabs overrideParams={{openai: {model: \"gpt-4o-mini\"}}} />\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# !pip install -qU \"langchain[openai]\"\n",
        "!pip install -qU \"langchain[groq]\""
      ],
      "metadata": {
        "id": "DL8oW2_t2kR5",
        "outputId": "a8b213c1-3596-44e7-ef56-60357dbde7c1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "DL8oW2_t2kR5",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/122.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━\u001b[0m \u001b[32m112.6/122.2 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m122.2/122.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4b41234",
      "metadata": {
        "id": "e4b41234"
      },
      "outputs": [],
      "source": [
        "# | output: false\n",
        "# | echo: false\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model=\"gpt-4o-mini\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "if not os.environ.get(\"GROQ_API_KEY\"):\n",
        "  os.environ[\"GROQ_API_KEY\"] = userdata.get('groq')\n",
        "\n",
        "from langchain.chat_models import init_chat_model\n",
        "\n",
        "model = init_chat_model(\"llama3-8b-8192\", model_provider=\"groq\")"
      ],
      "metadata": {
        "id": "oLBMPtW627Dp"
      },
      "id": "oLBMPtW627Dp",
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "ca5642ff",
      "metadata": {
        "id": "ca5642ff"
      },
      "source": [
        "Let's first use the model directly. [ChatModels](/docs/concepts/chat_models) are instances of LangChain [Runnables](/docs/concepts/runnables/), which means they expose a standard interface for interacting with them. To simply call the model, we can pass in a list of [messages](/docs/concepts/messages/) to the `.invoke` method.\n",
        "\n",
        "我们首先直接使用模型。ChatModel 是 LangChain Runnables 的实例，这意味着它们公开了用于与它们交互的标准接口。要简单地调用模型，我们可以将消息列表传递给 .invoke 方法。"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "1b2481f0",
      "metadata": {
        "id": "1b2481f0",
        "outputId": "cb73e45e-c1fb-467e-ca5c-ab89f68e4ddf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Ciao!', additional_kwargs={}, response_metadata={'token_usage': {'completion_tokens': 4, 'prompt_tokens': 24, 'total_tokens': 28, 'completion_time': 0.003333333, 'prompt_time': 0.003384447, 'queue_time': 0.019616012000000002, 'total_time': 0.00671778}, 'model_name': 'llama3-8b-8192', 'system_fingerprint': 'fp_6a6771ae9c', 'finish_reason': 'stop', 'logprobs': None}, id='run-bf8e93d1-e787-4f2e-be43-6ad1e11584f8-0', usage_metadata={'input_tokens': 24, 'output_tokens': 4, 'total_tokens': 28})"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(\"Translate the following from English into Italian\"),\n",
        "    HumanMessage(\"hi!\"),\n",
        "]\n",
        "\n",
        "model.invoke(messages)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f83373db",
      "metadata": {
        "id": "f83373db"
      },
      "source": [
        ":::tip\n",
        "\n",
        "If we've enabled LangSmith, we can see that this run is logged to LangSmith, and can see the [LangSmith trace](https://smith.langchain.com/public/88baa0b2-7c1a-4d09-ba30-a47985dde2ea/r). The LangSmith trace reports [token](/docs/concepts/tokens/) usage information, latency, [standard model parameters](/docs/concepts/chat_models/#standard-parameters) (such as temperature), and other information.\n",
        "\n",
        "如果我们启用了 LangSmith，我们可以看到此运行已记录到 LangSmith 中，并且可以看到 LangSmith 跟踪。LangSmith 跟踪报告令牌使用信息、延迟、标准模型参数（例如温度）和其他信息。\n",
        ":::\n",
        "\n",
        "Note that ChatModels receive [message](/docs/concepts/messages/) objects as input and generate message objects as output. In addition to text content, message objects convey conversational [roles](/docs/concepts/messages/#role) and hold important data, such as [tool calls](/docs/concepts/tool_calling/) and token usage counts.\n",
        "\n",
        "请注意，ChatModels 接收消息对象作为输入，并生成消息对象作为输出。除了文本内容之外，消息对象还传达会话角色并保存重要数据，例如工具调用和令牌使用计数。\n",
        "\n",
        "LangChain also supports chat model inputs via strings or [OpenAI format](/docs/concepts/messages/#openai-format). The following are equivalent:\n",
        "\n",
        "LangChain 还支持通过字符串或 OpenAI 格式输入聊天模型。以下是等效的：\n",
        "```python\n",
        "model.invoke(\"Hello\")\n",
        "\n",
        "model.invoke([{\"role\": \"user\", \"content\": \"Hello\"}])\n",
        "\n",
        "model.invoke([HumanMessage(\"Hello\")])\n",
        "```\n",
        "\n",
        "### Streaming 流\n",
        "\n",
        "Because chat models are [Runnables](/docs/concepts/runnables/), they expose a standard interface that includes async and streaming modes of invocation. This allows us to stream individual tokens from a chat model:\n",
        "\n",
        "由于聊天模型是 Runnables，因此它们公开了一个标准接口，其中包括异步和流式调用模式。这允许我们从聊天模型中流式传输单个令牌："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "0abb0863-bee7-448d-b013-79d8db01e330",
      "metadata": {
        "id": "0abb0863-bee7-448d-b013-79d8db01e330",
        "outputId": "4cd15de8-cf9f-4d43-976c-571d86eef60c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "|C|iao|!||"
          ]
        }
      ],
      "source": [
        "for token in model.stream(messages):\n",
        "    print(token.content, end=\"|\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a5963141-468c-4570-8f2e-5f7cfb6eb3db",
      "metadata": {
        "id": "a5963141-468c-4570-8f2e-5f7cfb6eb3db"
      },
      "source": [
        "您可以在[本指南](/docs/how_to/chat_streaming/)中找到有关流式聊天模型输出的更多详细信息。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1ab8da31",
      "metadata": {
        "id": "1ab8da31"
      },
      "source": [
        "## Prompt Templates 提示模板\n",
        "\n",
        "现在，我们将消息列表直接传递到语言模型中。此消息列表来自何处？通常，它是由用户输入和应用程序逻辑的组合构建的。此应用程序逻辑通常采用原始用户输入，并将其转换为准备传递给语言模型的消息列表。常见转换包括添加系统消息或使用用户输入设置模板格式。\n",
        "\n",
        "[提示模板](/docs/concepts/prompt_templates/)是 LangChain 中的一个概念，旨在协助进行这种转换。它们接收原始用户输入并返回准备传递到语言模型的数据（提示）。\n",
        "\n",
        "让我们在这里创建一个提示模板。它将接受两个用户变量：\n",
        "\n",
        "- `language`: 要将文本翻译成的语言\n",
        "- `text`:  要翻译的文本"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "3e73cc20",
      "metadata": {
        "id": "3e73cc20"
      },
      "outputs": [],
      "source": [
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"Translate the following from English into {language}\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [(\"system\", system_template), (\"user\", \"{text}\")]\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e876c2a",
      "metadata": {
        "id": "7e876c2a"
      },
      "source": [
        "Note that `ChatPromptTemplate` supports multiple [message roles](/docs/concepts/messages/#role) in a single template. We format the `language` parameter into the system message, and the user `text` into a user message.\n",
        "\n",
        "请注意，ChatPromptTemplate 支持单个模板中的多个消息角色。我们将 language 参数格式化为系统消息，将用户文本格式化为用户消息。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9711ba6",
      "metadata": {
        "id": "d9711ba6"
      },
      "source": [
        "此提示模板的输入是字典。我们可以单独使用这个提示模板，看看它自己做了什么"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "id": "f781b3cb",
      "metadata": {
        "id": "f781b3cb",
        "outputId": "3c4081ef-3566-4ea4-9c05-6792ca2b9bcd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}), HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "prompt = prompt_template.invoke({\"language\": \"Italian\", \"text\": \"hi!\"})\n",
        "\n",
        "prompt"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1a49ba9e",
      "metadata": {
        "id": "1a49ba9e"
      },
      "source": [
        "我们可以看到，它返回一个由两条消息组成的`ChatPromptValue`。如果我们想直接访问消息，我们会这样做："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "2159b619",
      "metadata": {
        "id": "2159b619",
        "outputId": "2e53a119-d304-440b-85ca-b04eb690e69a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content='Translate the following from English into Italian', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='hi!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "prompt.to_messages()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "47e70ee6-f0e0-4ae0-a290-002799ebf828",
      "metadata": {
        "id": "47e70ee6-f0e0-4ae0-a290-002799ebf828"
      },
      "source": [
        "最后，我们可以在格式化的提示符上调用 chat 模型："
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "3a509d8c-e122-4641-b9ee-91bc23aa155a",
      "metadata": {
        "id": "3a509d8c-e122-4641-b9ee-91bc23aa155a",
        "outputId": "ce082541-14b2-4eea-cabb-8c04b9aa5bec",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Ciao!\n"
          ]
        }
      ],
      "source": [
        "response = model.invoke(prompt)\n",
        "print(response.content)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d7f0bf25-6efb-4853-9a8f-242f2855c84a",
      "metadata": {
        "id": "d7f0bf25-6efb-4853-9a8f-242f2855c84a"
      },
      "source": [
        ":::tip\n",
        "Message `content` can contain both text and [content blocks](/docs/concepts/messages/#aimessage) with additional structure. See [this guide](/docs/how_to/output_parser_string/) for more information.\n",
        "\n",
        "消息内容可以包含具有附加结构的文本块和内容块。有关更多信息，请参阅本指南。\n",
        ":::\n",
        "\n",
        "If we take a look at the [LangSmith trace](https://smith.langchain.com/public/3ccc2d5e-2869-467b-95d6-33a577df99a2/r), we can see exactly what prompt the chat model receives, along with [token](/docs/concepts/tokens/) usage information, latency, [standard model parameters](/docs/concepts/chat_models/#standard-parameters) (such as temperature), and other information.\n",
        "\n",
        "如果我们看一下 LangSmith 跟踪，我们可以准确地看到聊天模型收到了什么提示，以及令牌使用信息、延迟、标准模型参数（如温度）和其他信息。"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "befdb168",
      "metadata": {
        "id": "befdb168"
      },
      "source": [
        "## Conclusion 结论\n",
        "\n",
        "就是这样！在本教程中，您学习了如何创建您的第一个简单LLM应用程序。您已经学习了如何使用语言模型、如何创建提示模板，以及如何在使用 LangSmith 创建的应用程序中实现出色的可观测性。\n",
        "\n",
        "这只是触及了您想要学习成为熟练 AI 工程师的皮毛。幸运的是 - 我们还有很多其他资源！\n",
        "\n",
        "For 要进一步了解 LangChain 的核心概念，我们提供了详细的[概念指南](/docs/concepts).\n",
        "\n",
        "如果您对这些概念有更具体的问题，请查看作指南的以下部分：\n",
        "\n",
        "- [聊天模型](/docs/how_to/#chat-models)\n",
        "- [提示模板](/docs/how_to/#prompt-templates)\n",
        "\n",
        "And the LangSmith docs:\n",
        "\n",
        "- [LangSmith](https://docs.smith.langchain.com)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.4"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}